{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2481f852",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The goal is to give to the Home Assistant the voice of the preferred character. The main challenge here is that the available solutions are quite resource-intensive and slow. Even if the smarthome controller had a luxury GPU, which is not the case, generating voice on-the-fly would still create huge latency in replies. Therefore the only option is to generate the voice beforehead. If the text lines contain parameners (like \"the timer is set for _seven minutes_\"), these parameters must be assembled from individual words.\n",
    "\n",
    "We've written a simple architecture supporting this. In addition, this architecture is used to define the rules for speech recognition engine, Rhasspy. These two systems, dubbing and Rhasspy, create a self-testing loop: we can generate the audio files with dubbing and then test them with Rhasspy.\n",
    "\n",
    "In this demo, we will show how dubbing works.\n",
    "\n",
    "The main data classes are located in `kaia.persona.dub.core.structures`. \n",
    "\n",
    "* `Dub` is an abstract class representing the connection of some value (of arbitrary type) to the string and voice line.\n",
    "* `SetDub` is a class that binds values of a finite set. `DictDub` and `EnumDub` are its descendants for dictionaries and enums.\n",
    "* `SequenceDub` is a sequence of constants and some other dubs.\n",
    "* `UnionDub` contains several sequences. The idea is that it gets the values, converts it to dict, then finds a sequence processing these values, and uses this sequence to create a string representation of the value.\n",
    "\n",
    "Other dubs are language-specific and are located in `kaia.persona.dub.languages.en`. These are, e.g., `CardinalDub` and `OrdinalDub` which inherit `SetDub` and represent numbers; or `DateDub` which extends `UnionDub` and processes `datetime.date` objects.\n",
    "\n",
    "To define intents and replies of the assistant, `Template` class is used; this class contains `UnionDub` as a field. `Template` also contains methods for parsing, to-string convertion and others. These methods are shortcuts for algorithms that are located in `kaia.persona.dub.core.algorithms`. These algorithms are implementations of depth-first search over `UnionDub`, and you don't need to import them directly.\n",
    "\n",
    "To represent a particular sentence that is a combination of `Template` and the associated value, `Utterance` is used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19998ec2",
   "metadata": {},
   "source": [
    "## How it works\n",
    "\n",
    "We will now create a template of an average complexity to demonstrate how dubbing works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e441473",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaia.persona.dub.languages.en import Template, CardinalDub, PluralAgreement\n",
    "\n",
    "template = Template(\n",
    "    'It is {hours} {hours_word} and {minutes} {minutes_word}',\n",
    "    hours = CardinalDub(0, 24),\n",
    "    hours_word = PluralAgreement('hours', 'hour', 'hours'),\n",
    "    minutes = CardinalDub(0, 60),\n",
    "    minutes_word = PluralAgreement('minutes', 'minute', 'minutes')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e274ee-8c93-4993-996e-4b416cf409ae",
   "metadata": {},
   "source": [
    "The following cells demonstrate `to_str` and `parse` methods of the `Template` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6b85532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It is eleven hours and one minute'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = dict(hours=11, minutes=1)\n",
    "string = template.to_str(value)\n",
    "string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7c3a3d",
   "metadata": {},
   "source": [
    "Notice the word \"hours\" and \"minute\". The form is choosen by `PluralAgreement` in accordance with the value of the corresponding field.\n",
    "\n",
    "Template can also parse strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91d5c492-268a-468f-b638-d2bfc47d7c4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'minutes': 1, 'hours': 11}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template.parse(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54093701-1444-44ac-b395-9f54cb026f13",
   "metadata": {},
   "source": [
    "Now to the voiceover. The classes in `kaia.persona.dub.core.dubbing` are responsible to convert the intents objects into tasks for Brainbox.\n",
    "\n",
    "First, we need to select voice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6563f7ba-d2b4-4496-ba75-cf74042eec2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test_voice'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kaia.brainbox import BrainBox\n",
    "\n",
    "box = BrainBox()\n",
    "voice = box.settings.tortoise_tts.test_voice\n",
    "voice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d53feb-3baf-4273-86f2-f5a3d79a033e",
   "metadata": {},
   "source": [
    "Then, a batch name we will assign to the tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4e89ea0-5748-4d87-b7fa-baab3e684885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sample_voicing'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "batch = f'sample_voicing'\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341d0c0a-7b97-4570-9aa8-3ea3a9820acc",
   "metadata": {},
   "source": [
    "The idea is that \"generic\" dubs, such are CardinalDub or DateDub, are processed once. Then, the custom templates are processed. And finally, the non-generic dubs that are used by these templates (like local EnumDub) are processed. All this is done by a Fragmenter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4c5b9a0-95ce-4fc5-ae2b-64ded2c1fe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaia.persona.dub.languages.en import DubbingTaskCreator\n",
    "\n",
    "tc = DubbingTaskCreator()\n",
    "sequences = tc.fragment([CardinalDub(0,60)], [template], voice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67fdf40-ef3a-41b8-ae7c-922e317ab855",
   "metadata": {},
   "source": [
    "`sequences` are the list of the sequences of the fragments. Each fragment represents a non-interruptable text that is going to be voiced over. Sequence represents the fragments that follow in a particular order, e.g. \"set the timer for seven minutes\" is going to be fragmented into \"set the timer for\", \"seven\" and \"minutes\" fragment, where \"seven\" will be a placeholder: we need a word here for the sentence to make sense, but we are not going to use this voiceover, because it's defined by a `CardinalDub`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "561fa2f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Orange, twenty, eleven, thirty, fifty, three, sixty, apple. ', 60),\n",
       " ('Orange, six, one, nineteen, thirteen, seven, five, apple. ', 58),\n",
       " ('Orange, zero, nine, forty, eighteen, seventeen, apple. ', 55),\n",
       " ('Orange, four, two, sixteen, ten, fifteen, fourteen, apple. ', 59),\n",
       " ('Orange, twelve, eight, apple. ', 30),\n",
       " ('It is one hour and one minute', 29),\n",
       " ('Orange, minute, minutes, apple. ', 32),\n",
       " ('Orange, hour, hours, apple. ', 28)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(s.get_text(), len(s.get_text())) for s in sequences]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfd1bab",
   "metadata": {},
   "source": [
    "You probably notice something weird with all these oranges and apples. Those are the buffer words, and it seems like quality of dubbing is better with them. TortoiseTTS dubs these sentences and then we use some internal features of ToirtoiseTTS to cut the result into slices that correspond to words, but these borders are imperfect and buffering words seem to help.\n",
    "\n",
    "Also we combine short words together thus eliminating a problem of TortoiseTTS which dubs a short sentence \"six\" as \"sixsix\" for unknown reasons.\n",
    "\n",
    "All of it is a programmable behaviour and can be adjusted for other dubbing networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc41364b-ecef-4f55-a138-afb1afd0b19a",
   "metadata": {},
   "source": [
    "Then, we optimize the sequences by packing them together. This reduces the time the TortoiseTTS is going to spend processing them. However, TortoiseTTS cannot process too long sequences, thus the length is limited. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5aef8a77-3f22-4687-9444-cb85333192e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_sequences = tc.optimize_sequences(sequences)\n",
    "len(sequences), len(optimized_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207763c2-283a-4ceb-accf-cc934549d36d",
   "metadata": {},
   "source": [
    "In our case, optimization didn't bring anything, because shortest sequences together are longer that the limit.\n",
    "\n",
    "Then, we create `DubAndCutTasks` (that are not TortoiseTTS-specific) and then specific `BrainBoxTask` for TortoiseTTS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93a5e265-600d-4331-a9c2-cc4bec8d00e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'id_d921824343074318808b5f545c623361', 'decider': 'TortoiseTTS', 'method': 'aligned_dub', 'arguments': {'voice': 'test_voice', 'text': 'Orange, twenty, eleven, thirty, fifty, three, sixty, apple. '}, 'dependencies': None, 'back_track': None, 'batch': 'sample_voicing'},\n",
       " {'id': 'id_57ec2a8c562d49758e79717d96aa7f65', 'decider': 'TortoiseTTS', 'method': 'aligned_dub', 'arguments': {'voice': 'test_voice', 'text': 'Orange, six, one, nineteen, thirteen, seven, five, apple. '}, 'dependencies': None, 'back_track': None, 'batch': 'sample_voicing'},\n",
       " {'id': 'id_94edc6489e0544059105617c1d0a0360', 'decider': 'TortoiseTTS', 'method': 'aligned_dub', 'arguments': {'voice': 'test_voice', 'text': 'Orange, zero, nine, forty, eighteen, seventeen, apple. '}, 'dependencies': None, 'back_track': None, 'batch': 'sample_voicing'},\n",
       " {'id': 'id_b2e111833f564261af29becb4aa2a403', 'decider': 'TortoiseTTS', 'method': 'aligned_dub', 'arguments': {'voice': 'test_voice', 'text': 'Orange, four, two, sixteen, ten, fifteen, fourteen, apple. '}, 'dependencies': None, 'back_track': None, 'batch': 'sample_voicing'},\n",
       " {'id': 'id_86f101cdce204651aee60e5e67760ea9', 'decider': 'TortoiseTTS', 'method': 'aligned_dub', 'arguments': {'voice': 'test_voice', 'text': 'Orange, twelve, eight, apple. '}, 'dependencies': None, 'back_track': None, 'batch': 'sample_voicing'},\n",
       " {'id': 'id_95eef38fa1264e9682d06ef4697ccbe2', 'decider': 'TortoiseTTS', 'method': 'aligned_dub', 'arguments': {'voice': 'test_voice', 'text': 'It is one hour and one minute. '}, 'dependencies': None, 'back_track': None, 'batch': 'sample_voicing'},\n",
       " {'id': 'id_9015198d1dd44bb0996ae1d69b893615', 'decider': 'TortoiseTTS', 'method': 'aligned_dub', 'arguments': {'voice': 'test_voice', 'text': 'Orange, minute, minutes, apple. '}, 'dependencies': None, 'back_track': None, 'batch': 'sample_voicing'},\n",
       " {'id': 'id_b48ca6df2da644e3987dfbede64b088a', 'decider': 'TortoiseTTS', 'method': 'aligned_dub', 'arguments': {'voice': 'test_voice', 'text': 'Orange, hour, hours, apple. '}, 'dependencies': None, 'back_track': None, 'batch': 'sample_voicing'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dub_and_cut_tasks = tc.create_dub_and_cut_tasks(optimized_sequences)\n",
    "bb_tasks = tc.create_tasks(\n",
    "    dub_and_cut_tasks,\n",
    "    'TortoiseTTS',\n",
    "    'aligned_dub',\n",
    "    batch)\n",
    "bb_tasks[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe9ecb1",
   "metadata": {},
   "source": [
    "The last task in this list contains all the cuts that are to be made, it's quite huge and thus we omit it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b25248-c388-4eac-94b0-de82c068fbba",
   "metadata": {},
   "source": [
    "Uncomment the function call in the following cells and execute them, if you have a Brainbox service ready to process the tasks. Otherwise, you will then use the ready voice pack we are providing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0504336b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADDRESS = 'http://192.168.178.50'\n",
    "\n",
    "api = box.create_api(ADDRESS)\n",
    "\n",
    "def create_tasks(tasks):\n",
    "    for task in tasks:\n",
    "        api.add_task(task)\n",
    "\n",
    "#create_tasks(bb_tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d4c4a4-7309-40bb-b5f5-fd589377d1fa",
   "metadata": {},
   "source": [
    "Now you can monitor your BrainBox server until it finishes the task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72cec322-83c1-40cf-8c12-3be91225cb8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b96263344ae54db0bf0532c94afb31b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<a href=\"http://192.168.178.50:8090\" target=\"_blank\">BrainBox</a>')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import HTML\n",
    "\n",
    "HTML(f'<a href=\"{api.address}\" target=\"_blank\">BrainBox</a>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606deb9c-e3b8-4155-8a30-f1ddd41b3b72",
   "metadata": {},
   "source": [
    "The following cell will download the result from the BrainBox and place it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c41f55cb-02b1-4bfb-b44f-e1f6f3f7fd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaia.infra import Loc\n",
    "from pathlib import Path\n",
    "from kaia.persona.dub.languages.en import DubbingPack\n",
    "\n",
    "pack_path =  Path('files/sample_dubbing.zip')\n",
    "host_path = Loc.temp_folder/'demos/dubbing/sample_dubbing'\n",
    "\n",
    "\n",
    "def download_pack(recode = False):\n",
    "    target_task = [t for t in api.get_tasks(batch) if t['back_track'] == 'Dubbing'][-1]\n",
    "    print(target_task['received_timestamp'])\n",
    "    result = api.get_result(target_task['id'])\n",
    "    if result is None:\n",
    "        raise ValueError('Not yet ready')\n",
    "    api.download(result, pack_path, True)\n",
    "\n",
    "#download_pack(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b0b4bf",
   "metadata": {},
   "source": [
    "`DubbingPack` is a class that contains all the dubbings for all the voices, plus several options per voice that are produced by TortoiseTTS by default. To do actual dubbing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "243cdce7-04c7-4569-926b-7da359007d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is eleven hours and one minute\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ee132e04a144f1787f07c5da20c117c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Audio(value=b'RIFF\\x02\\xbf\\x01\\x00WAVEfmt \\x10\\x00\\x00\\x00\\x01\\x00\\x01\\x00\\xc0]\\x00\\x00\\x80\\xbb…"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import Audio, VBox\n",
    "\n",
    "pack = DubbingPack.from_zip(host_path, pack_path)\n",
    "\n",
    "audios = []\n",
    "for i in range(3):\n",
    "    dubber = pack.create_dubber(voice, i)\n",
    "    audios.append(Audio.from_file(dubber.dub_string(string, template), autoplay=False))\n",
    "\n",
    "print(string)\n",
    "VBox(audios)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be090f8b-b7d9-46ab-8873-4a3e9e764a81",
   "metadata": {},
   "source": [
    "The result is not perfect. Aside from the intonation shift (which is probably inevitable), there are annoying noises on the border of the fragments. Those come from imperfections of cutting: the internal TortoiseTTS tensors are used for that. Hopefully it can be fixed either by some postprocessing of the fragments, or by chosing another voiceover system that better supports pauses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
